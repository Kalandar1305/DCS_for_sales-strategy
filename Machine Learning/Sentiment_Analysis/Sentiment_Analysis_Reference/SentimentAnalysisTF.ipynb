{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3e0b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "num_gpus_available = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", num_gpus_available)\n",
    "assert num_gpus_available > 0\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac69549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {data: {customer_id: (), helpful_votes: (), marketplace: (), product_category: (), product_id: (), product_parent: (), product_title: (), review_body: (), review_date: (), review_headline: (), review_id: (), star_rating: (), total_votes: (), verified_purchase: (), vine: ()}}, types: {data: {customer_id: tf.string, helpful_votes: tf.int32, marketplace: tf.string, product_category: tf.string, product_id: tf.string, product_parent: tf.string, product_title: tf.string, review_body: tf.string, review_date: tf.string, review_headline: tf.string, review_id: tf.string, star_rating: tf.int32, total_votes: tf.int32, verified_purchase: tf.int64, vine: tf.int64}}>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds = tfds.load('amazon_us_reviews/Mobile_Electronics_v1_00', split='train', shuffle_files=True)\n",
    "assert isinstance(ds, tf.data.Dataset)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d3046b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8c1853bf56e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/as_dataframe.py\u001b[0m in \u001b[0;36mas_dataframe\u001b[0;34m(ds, ds_info)\u001b[0m\n\u001b[1;32m    219\u001b[0m   \u001b[0;31m# order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m   \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_row_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyledDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_fn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_fn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/as_dataframe.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    219\u001b[0m   \u001b[0;31m# order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m   \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_row_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m   \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStyledDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_fn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_fn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/dataset_utils.py\u001b[0m in \u001b[0;36m_eager_dataset_iterator\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_eager_dataset_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNumpyElem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0mtree_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_elem_to_numpy_eager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tree/__init__.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   return unflatten_as(structures[0],\n\u001b[0;32m--> 435\u001b[0;31m                       [func(*args) for args in zip(*map(flatten, structures))])\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tree/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m   return unflatten_as(structures[0],\n\u001b[0;32m--> 435\u001b[0;31m                       [func(*args) for args in zip(*map(flatten, structures))])\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow_datasets/core/dataset_utils.py\u001b[0m in \u001b[0;36m_elem_to_numpy_eager\u001b[0;34m(tf_el)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;34m\"\"\"Converts a single element from tf to numpy.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_el\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_el\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error  # pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_el\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_el\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = tfds.as_dataframe(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bd65fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data/customer_id</th>\n",
       "      <th>data/helpful_votes</th>\n",
       "      <th>data/marketplace</th>\n",
       "      <th>data/product_category</th>\n",
       "      <th>data/product_id</th>\n",
       "      <th>data/product_parent</th>\n",
       "      <th>data/product_title</th>\n",
       "      <th>data/review_body</th>\n",
       "      <th>data/review_date</th>\n",
       "      <th>data/review_headline</th>\n",
       "      <th>data/review_id</th>\n",
       "      <th>data/star_rating</th>\n",
       "      <th>data/total_votes</th>\n",
       "      <th>data/verified_purchase</th>\n",
       "      <th>data/vine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'20980074'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'US'</td>\n",
       "      <td>b'Mobile_Electronics'</td>\n",
       "      <td>b'B00D1847NE'</td>\n",
       "      <td>b'274617424'</td>\n",
       "      <td>b'Teenage Mutant Ninja Turtles Boombox CD Play...</td>\n",
       "      <td>b'Does not work'</td>\n",
       "      <td>b'2015-01-09'</td>\n",
       "      <td>b'One Star'</td>\n",
       "      <td>b'R1OVS0D6SEXPW7'</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'779273'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'US'</td>\n",
       "      <td>b'Mobile_Electronics'</td>\n",
       "      <td>b'B00KMO6DYG'</td>\n",
       "      <td>b'397452138'</td>\n",
       "      <td>b'4 Gauge Amp Kit Amplifier Install Wiring Com...</td>\n",
       "      <td>b'This is a great wiring kit i used it to set ...</td>\n",
       "      <td>b'2015-08-06'</td>\n",
       "      <td>b'Great kit'</td>\n",
       "      <td>b'R9VSD0ET8FERB'</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'15410531'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'US'</td>\n",
       "      <td>b'Mobile_Electronics'</td>\n",
       "      <td>b'B000GWLL0K'</td>\n",
       "      <td>b'948304826'</td>\n",
       "      <td>b'Travel Wall Charger fits Creative Zen Vision...</td>\n",
       "      <td>b'It works great so much faster than USB charg...</td>\n",
       "      <td>b'2007-03-15'</td>\n",
       "      <td>b'A/C Charger for Creative Zen Vision M'</td>\n",
       "      <td>b'R3ISXCZHWLJLBH'</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'27389005'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'US'</td>\n",
       "      <td>b'Mobile_Electronics'</td>\n",
       "      <td>b'B008L3JE6Y'</td>\n",
       "      <td>b'466340015'</td>\n",
       "      <td>b'High Grade Robust 360\\xc2\\xb0 Adjustable Car...</td>\n",
       "      <td>b'This product was purchased to hold a monitor...</td>\n",
       "      <td>b'2013-07-30'</td>\n",
       "      <td>b'camera stand'</td>\n",
       "      <td>b'R1TWVUDOFJSQAW'</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'2663569'</td>\n",
       "      <td>0</td>\n",
       "      <td>b'US'</td>\n",
       "      <td>b'Mobile_Electronics'</td>\n",
       "      <td>b'B00GHZS4SC'</td>\n",
       "      <td>b'350592810'</td>\n",
       "      <td>b'HDE Multifunctional Bluetooth FM Audio Car K...</td>\n",
       "      <td>b\"it works but it has really bad sound quality...</td>\n",
       "      <td>b'2014-12-31'</td>\n",
       "      <td>b'bad sound quality'</td>\n",
       "      <td>b'R2PEOEUR1LP0GH'</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data/customer_id  data/helpful_votes data/marketplace  \\\n",
       "0      b'20980074'                   0            b'US'   \n",
       "1        b'779273'                   0            b'US'   \n",
       "2      b'15410531'                   0            b'US'   \n",
       "3      b'27389005'                   0            b'US'   \n",
       "4       b'2663569'                   0            b'US'   \n",
       "\n",
       "   data/product_category data/product_id data/product_parent  \\\n",
       "0  b'Mobile_Electronics'   b'B00D1847NE'        b'274617424'   \n",
       "1  b'Mobile_Electronics'   b'B00KMO6DYG'        b'397452138'   \n",
       "2  b'Mobile_Electronics'   b'B000GWLL0K'        b'948304826'   \n",
       "3  b'Mobile_Electronics'   b'B008L3JE6Y'        b'466340015'   \n",
       "4  b'Mobile_Electronics'   b'B00GHZS4SC'        b'350592810'   \n",
       "\n",
       "                                  data/product_title  \\\n",
       "0  b'Teenage Mutant Ninja Turtles Boombox CD Play...   \n",
       "1  b'4 Gauge Amp Kit Amplifier Install Wiring Com...   \n",
       "2  b'Travel Wall Charger fits Creative Zen Vision...   \n",
       "3  b'High Grade Robust 360\\xc2\\xb0 Adjustable Car...   \n",
       "4  b'HDE Multifunctional Bluetooth FM Audio Car K...   \n",
       "\n",
       "                                    data/review_body data/review_date  \\\n",
       "0                                   b'Does not work'    b'2015-01-09'   \n",
       "1  b'This is a great wiring kit i used it to set ...    b'2015-08-06'   \n",
       "2  b'It works great so much faster than USB charg...    b'2007-03-15'   \n",
       "3  b'This product was purchased to hold a monitor...    b'2013-07-30'   \n",
       "4  b\"it works but it has really bad sound quality...    b'2014-12-31'   \n",
       "\n",
       "                       data/review_headline     data/review_id  \\\n",
       "0                               b'One Star'  b'R1OVS0D6SEXPW7'   \n",
       "1                              b'Great kit'   b'R9VSD0ET8FERB'   \n",
       "2  b'A/C Charger for Creative Zen Vision M'  b'R3ISXCZHWLJLBH'   \n",
       "3                           b'camera stand'  b'R1TWVUDOFJSQAW'   \n",
       "4                      b'bad sound quality'  b'R2PEOEUR1LP0GH'   \n",
       "\n",
       "   data/star_rating  data/total_votes  data/verified_purchase  data/vine  \n",
       "0                 1                 0                       0          1  \n",
       "1                 4                 0                       0          1  \n",
       "2                 5                 0                       0          1  \n",
       "3                 5                 0                       0          1  \n",
       "4                 3                 0                       0          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76363bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentiment\"] = df[\"data/star_rating\"].apply(lambda score: \"positive\" if score >= 3 else \"negative\")\n",
    "df['Sentiment'] = df['Sentiment'].map({'positive':1, 'negative':0})\n",
    "df['short_review'] =df['data/review_body'].str.decode(\"utf-8\")\n",
    "df = df[[\"short_review\", \"Sentiment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabec149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-1d112508c1d6>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(df.tail(n).index,\n"
     ]
    }
   ],
   "source": [
    "n = 54975\n",
    "df.drop(df.tail(n).index,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d255305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['short_review'].values.tolist()\n",
    "labels = df['Sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa9f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a715cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(reviews, labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a1754f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 3856, 2023, 2039, 2005, 3909, 1996, 17350, 2692, 2278, 1999, 5887, 2015, 2088, 1010, 2000, 2393, 2007, 4363, 2650, 1997, 3260, 27918, 2015, 1998, 1046, 2696, 2278, 4455, 1012, 2009, 2038, 2499, 2307, 1010, 1998, 1996, 2210, 26279, 1010, 2066, 1996, 7279, 9111, 1998, 10245, 3614, 21628, 2031, 2272, 1999, 2200, 18801, 1012, 2009, 2003, 2092, 2328, 1010, 1998, 2145, 3065, 2053, 5751, 1997, 4929, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([training_sentences[0]], truncation=True,padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56725401",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(training_sentences,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "val_encodings = tokenizer(validation_sentences,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(train_encodings),\n",
    "                            training_labels\n",
    "                            ))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(val_encodings),\n",
    "                            validation_labels\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b847e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.038468360900878906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)\"tf_model.h5\";",
       "rate": null,
       "total": 363423424,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb9ef5943d94ed681f2d7fc5c455e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18555335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x7fced4bae310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x7fced4bae310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function input_processing at 0x7fced4bae310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py:376: FutureWarning: The old compute_loss method is deprecated as it conflicts with the Keras compute_loss method added in TF 2.8. If you want the original HF compute_loss, please call hf_compute_loss() instead. From TF versions >= 2.8, or Transformers versions >= 5, calling compute_loss() will get the Keras method instead.\n",
      "  return py_builtins.overload_of(f)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 1634s 647ms/step - loss: 0.2110 - accuracy: 0.9114 - val_loss: 0.1813 - val_accuracy: 0.9223\n",
      "Epoch 2/2\n",
      "2500/2500 [==============================] - 1616s 646ms/step - loss: 0.1330 - accuracy: 0.9477 - val_loss: 0.2274 - val_accuracy: 0.9203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcdb935b280>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
    "model.fit(train_dataset.shuffle(100).batch(16),\n",
    "          epochs=2,\n",
    "          batch_size=16,\n",
    "          validation_data=val_dataset.shuffle(100).batch(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acd66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at ./sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\"./sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f535ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"\"\n",
    "predict_input = tokenizer.encode(test_sentence,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")\n",
    "tf_output = loaded_model.predict(predict_input)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive']\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f87c8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 14), dtype=int32, numpy=\n",
       "array([[ 101, 1996, 4031, 2003, 2205, 2204, 2008, 1045, 2097, 2025, 4965,\n",
       "        2009, 2153,  102]], dtype=int32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74f2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x7f61f606eb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x7f61f606eb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "Negative\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600f1c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
