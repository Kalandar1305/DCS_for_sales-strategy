{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets with Emojis\n",
    "This is method 2.\n",
    "Where we use a dataset contatining both utf-8 emoticons and tweets texts. We will be training it using naive bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data\n",
    "we will be importing the dataset that we processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emojis = pd.read_csv('dataset/15_emoticon_data.csv')\n",
    "df_tweets = pd.read_csv('dataset/1k_data_emoji_tweets_senti_posneg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Unicode codepoint</th>\n",
       "      <th>Unicode name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>üòç</td>\n",
       "      <td>0x1f60d</td>\n",
       "      <td>SMILING FACE WITH HEART-SHAPED EYES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>üò≠</td>\n",
       "      <td>0x1f62d</td>\n",
       "      <td>LOUDLY CRYING FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>üòò</td>\n",
       "      <td>0x1f618</td>\n",
       "      <td>FACE THROWING A KISS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>üòä</td>\n",
       "      <td>0x1f60a</td>\n",
       "      <td>SMILING FACE WITH SMILING EYES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>üòÅ</td>\n",
       "      <td>0x1f601</td>\n",
       "      <td>GRINNING FACE WITH SMILING EYES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>üòâ</td>\n",
       "      <td>0x1f609</td>\n",
       "      <td>WINKING FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>üòÑ</td>\n",
       "      <td>0x1f604</td>\n",
       "      <td>SMILING FACE WITH OPEN MOUTH AND SMILING EYES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>üòí</td>\n",
       "      <td>0x1f612</td>\n",
       "      <td>UNAMUSED FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>üòî</td>\n",
       "      <td>0x1f614</td>\n",
       "      <td>PENSIVE FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>üò¢</td>\n",
       "      <td>0x1f622</td>\n",
       "      <td>CRYING FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>üòÜ</td>\n",
       "      <td>0x1f606</td>\n",
       "      <td>SMILING FACE WITH OPEN MOUTH AND TIGHTLY-CLOSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>üòÄ</td>\n",
       "      <td>0x1f600</td>\n",
       "      <td>GRINNING FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>üòê</td>\n",
       "      <td>0x1f610</td>\n",
       "      <td>NEUTRAL FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>üòõ</td>\n",
       "      <td>0x1f61b</td>\n",
       "      <td>FACE WITH STUCK-OUT TONGUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>üò≤</td>\n",
       "      <td>0x1f632</td>\n",
       "      <td>ASTONISHED FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>üòß</td>\n",
       "      <td>0x1f627</td>\n",
       "      <td>ANGUISHED FACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0 Emoji Unicode codepoint  \\\n",
       "0            0     üòç           0x1f60d   \n",
       "1            1     üò≠           0x1f62d   \n",
       "2            2     üòò           0x1f618   \n",
       "3            3     üòä           0x1f60a   \n",
       "4            4     üòÅ           0x1f601   \n",
       "5            5     üòâ           0x1f609   \n",
       "6            6     üòÑ           0x1f604   \n",
       "7            7     üòí           0x1f612   \n",
       "8            8     üòî           0x1f614   \n",
       "9            9     üò¢           0x1f622   \n",
       "10          10     üòÜ           0x1f606   \n",
       "11          11     üòÄ           0x1f600   \n",
       "12          12     üòê           0x1f610   \n",
       "13          13     üòõ           0x1f61b   \n",
       "14          14     üò≤           0x1f632   \n",
       "15          15     üòß           0x1f627   \n",
       "\n",
       "                                         Unicode name  \n",
       "0                 SMILING FACE WITH HEART-SHAPED EYES  \n",
       "1                                  LOUDLY CRYING FACE  \n",
       "2                                FACE THROWING A KISS  \n",
       "3                      SMILING FACE WITH SMILING EYES  \n",
       "4                     GRINNING FACE WITH SMILING EYES  \n",
       "5                                        WINKING FACE  \n",
       "6       SMILING FACE WITH OPEN MOUTH AND SMILING EYES  \n",
       "7                                       UNAMUSED FACE  \n",
       "8                                        PENSIVE FACE  \n",
       "9                                         CRYING FACE  \n",
       "10  SMILING FACE WITH OPEN MOUTH AND TIGHTLY-CLOSE...  \n",
       "11                                      GRINNING FACE  \n",
       "12                                       NEUTRAL FACE  \n",
       "13                         FACE WITH STUCK-OUT TONGUE  \n",
       "14                                    ASTONISHED FACE  \n",
       "15                                     ANGUISHED FACE  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the emojis used in the 1k tweet data\n",
    "# does not have sentiment cause the naive bayes will assign the sentiment automatically\n",
    "df_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One year ago today üòß .1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>keep smiling happy.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>It's hard to imagine anyone but Robin üòß but st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Good luck to Rich riding for great project in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>He didn't play for a year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>0</td>\n",
       "      <td>Maa ki kuss tumhari. Now take this bullshit yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>0</td>\n",
       "      <td>Pozuelo (formerly of Swans) and Suso (Liverpoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>0</td>\n",
       "      <td>Louis_Tomlinson follow me please? üòß</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>1</td>\n",
       "      <td>you know what i think? you look exceptional fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>Scout photobombed my bluebells photo üòî</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  sentiment                                               post\n",
       "0             0          0                            One year ago today üòß .1\n",
       "1             1          1                               keep smiling happy.1\n",
       "2             2          0  It's hard to imagine anyone but Robin üòß but st...\n",
       "3             3          1  Good luck to Rich riding for great project in ...\n",
       "4             4          1                          He didn't play for a year\n",
       "..          ...        ...                                                ...\n",
       "995         995          0  Maa ki kuss tumhari. Now take this bullshit yo...\n",
       "996         996          0  Pozuelo (formerly of Swans) and Suso (Liverpoo...\n",
       "997         997          0                Louis_Tomlinson follow me please? üòß\n",
       "998         998          1  you know what i think? you look exceptional fo...\n",
       "999         999          0             Scout photobombed my bluebells photo üòî\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Methods for Sentiment Analysis:\n",
    "1. this is method 2, simply using naive bayes to train the dataset\n",
    "    - pros : easiest to do, just focus on the naive bayes(NB) classifier, less preprocessing\n",
    "    - cons : the sentiments of emojis might not be accurate since it is based on the analysis of the given data to the NB classifier\n",
    "    \n",
    "2. another method, converting the emoticons to words first, before training the data\n",
    "    - pros : adds more vocabulary to the NB classifier, might have better sentiment analysis\n",
    "    - cons : additional data preprocessing, plus the cons of method above\n",
    "    \n",
    "3. can use the old method (method 1), separate text and emoji\n",
    "    - pros : emoji sentiment is based on the emoji data, thus will not be affected by the NB classifier\n",
    "    - cons : emoji sentiment might strongly affect the sentiment analysis (more emoji == less text sentiment influence). More data processing cause of text and symbol separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 768.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     ------------------------------------ 267.8/267.8 kB 968.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2022.10.31\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF vectorizer\n",
    "stopset = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True,\n",
    "                            strip_accents='ascii', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets2 = df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000, 2035)\n",
      "1000 observations X 2035 unique words\n"
     ]
    }
   ],
   "source": [
    "# dependent variable will be linked as:\n",
    "# 0 = negative, 1 = positive\n",
    "y = df_tweets2.sentiment\n",
    "# convert 'sentence' from text to features\n",
    "X = vectorizer.fit_transform(df_tweets2.post)\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "print(f'{X.shape[0]} observations X {X.shape[1]} unique words')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7648076923076923"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Train Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=None)\n",
    "\n",
    "# we will train a naive bayes classifier\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "# clf = naive_bayes.BernoulliNB()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# test our models accuracy\n",
    "roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build tweet sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(s_input = 'üòß I hate sentiment analysis üòß'):\n",
    "    # turn input into array\n",
    "    input_array= np.array([s_input])\n",
    "\n",
    "    # vectorize the input\n",
    "    input_vector = vectorizer.transform(input_array)\n",
    "    # predict the score of vector\n",
    "    \n",
    "    pred_senti = clf.predict(input_vector)\n",
    "\n",
    "    return pred_senti[0]\n",
    "print(get_sentiment())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_senti_status(test):\n",
    "    print('========================================')\n",
    "    print(f'Your input is \"{test}\" \\n')\n",
    "    sentiment = get_sentiment(test)\n",
    "    sentiment = 'Positive' if sentiment == 1 else 'Negative'\n",
    "    print(f'\\nYour input is of \"{sentiment}\" sentiment'.upper())\n",
    "    print('========================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòç I love sentiment analysis üòä\n"
     ]
    }
   ],
   "source": [
    "# for text area\n",
    "l = widgets.Layout(flex='0 1 auto', height='50px',width='auto')\n",
    "post_tweet = widgets.Textarea(value='üòç I love sentiment analysis üòä', layout=l)\n",
    "print(post_tweet.value)\n",
    "# for button\n",
    "button = widgets.Button(description=\"Say your Sentiments!\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_tweet_clicked(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        output.layout={'border': '1px solid black'}\n",
    "        print_senti_status(post_tweet.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45692b2dd67d462992dd8b94fdc878f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='üòç I love sentiment analysis üòä', layout=Layout(flex='0 1 auto', height='50px', width='auto'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ab515437f741f28e03b727a7cabe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Say your Sentiments!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c705054e9b604a00b9fc4f3894575b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# type post below\n",
    "display(post_tweet,button, output)\n",
    "button.on_click(on_tweet_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01126bec2af240dbbef20c43e62ee338": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "08f7ad18d03045a38291679244e628de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "border": "1px solid black"
      }
     },
     "2be717dc16e3409f9d03188453b31016": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Say your Sentiments!",
       "layout": "IPY_MODEL_01126bec2af240dbbef20c43e62ee338",
       "style": "IPY_MODEL_635f418e5dda4d3c923e6a78c90ae8b5"
      }
     },
     "635f418e5dda4d3c923e6a78c90ae8b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "6bd9d0aa1d3643f68a1957a44f057b0f": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_08f7ad18d03045a38291679244e628de",
       "outputs": [
        {
         "name": "stdout",
         "output_type": "stream",
         "text": "========================================\nYour input is \"üòç I love sentiment analysis üòä\" \n\n\nYOUR INPUT IS OF \"POSITIVE\" SENTIMENT\n========================================\n"
        }
       ]
      }
     },
     "a2dfb9c3644443b38b1d7df425137407": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "aec549cb2b5e42c193064abc32c8353b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "0 1 auto",
       "height": "50px",
       "width": "auto"
      }
     },
     "c4d27f9bb4b1452993c10f291fcc5c80": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ce9d51322ec448e18dd6ede085b9aeb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "TextareaModel",
      "state": {
       "layout": "IPY_MODEL_aec549cb2b5e42c193064abc32c8353b",
       "style": "IPY_MODEL_a2dfb9c3644443b38b1d7df425137407",
       "value": "üòç I love sentiment analysis üòä"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
