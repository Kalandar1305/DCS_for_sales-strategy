{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download(\"stopwords\")\n",
    "download(\"wordnet\")\n",
    "download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Crossvalidation RF\") \\\n",
    "    .master('spark://0.0.0.0:7077') \\\n",
    "    .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.executor.memory\", \"25g\")\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"false\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"datasets/sentiment_data_trim.csv\", header=True, inferSchema=True)\n",
    "df = df.drop(\"_c0\")\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df = df.withColumn(\"Reviews\", concat_ws(\" \", df.Review, df.Summary))\n",
    "df = df.drop(\"Review\", \"Summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def removeSpecChar(raw_text):\n",
    "    print(type(raw_text))\n",
    "    clean_SpecialChar = re.sub(\"[^a-zA-Z]\", \" \", raw_text)  \n",
    "    return clean_SpecialChar\n",
    "removeSpecialChar = udf(removeSpecChar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def get_wordnet_pos(tag):\n",
    "    from nltk.corpus import wordnet\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else: \n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizeScalar(sentance):\n",
    "    from nltk.tag import pos_tag\n",
    "    print(sentance)\n",
    "    tagged = pos_tag( [i for i in sentance if i])\n",
    "    lemmatized = []\n",
    "    for word, tag in tagged:\n",
    "        lemma = lemmatizer.lemmatize(word, pos = get_wordnet_pos(tag))\n",
    "        lemmatized.append(lemma)\n",
    "    return lemmatized\n",
    "lemmatize = udf(lemmatizeScalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "df = df.withColumn(\"Reviews\", removeSpecialChar(lower(df[\"Reviews\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Reviews\", outputCol=\"reviewtoken\")\n",
    "df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.corpus import stopwords\n",
    "remover = StopWordsRemover(inputCol=\"reviewtoken\", outputCol=\"reviewtokenfiltered\", stopWords=stopwords.words(\"english\"))\n",
    "df = remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"reviewtokenfiltered2\", lemmatize(df.reviewtokenfiltered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df = df.withColumn('reviewtokenfiltered2', expr(r\"regexp_extract_all(reviewtokenfiltered2, '(\\\\w+)', 1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"reviewtoken\", \"Reviews\",\"reviewtokenfiltered\")\n",
    "df = df.withColumnRenamed(\"reviewtokenfiltered2\", \"reviewtokenfiltered\")\n",
    "df = df.withColumnRenamed(\"Sentiment\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"reviewtokenfiltered\", outputCol=\"termFrequency\", numFeatures=37120)\n",
    "idf = IDF(inputCol=\"termFrequency\", outputCol=\"features\", )\n",
    "rf = RandomForestClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [hashingTF,idf,rf] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [20000,30000,40000])\\\n",
    "    .addGrid(rf.numTrees, [10,20,30]) \\\n",
    "    .addGrid(rf.minInfoGain, [0.0,0.1,0.01]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\", \"entropy\"])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossVal = CrossValidator(estimator=pipeline,\n",
    "                        estimatorParamMaps=param_grid,\n",
    "                        evaluator=BinaryClassificationEvaluator(),\n",
    "                        numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = crossVal.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "train_data,test_data=df.randomSplit([0.8,0.2],seed=123)\n",
    "trained_model=bestModel.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=trained_model.transform(test_data)\n",
    "area_under_curve = BinaryClassificationEvaluator().evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "preds_and_labels = predictions.select(['prediction','label']).withColumn('label', F.col('label').cast(FloatType())).orderBy('prediction')\n",
    "preds_and_labels = preds_and_labels.select(['prediction','label'])\n",
    "metrics = BinaryClassificationMetrics(preds_and_labels.rdd.map(tuple))\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
